
input {
    kafka {
        bootstrap_servers => ["broker001:9092"]
        auto_offset_reset => "latest"
        consumer_threads => 3 # 3个消费线程，默认是1个
        topics => ["log","log-collect"]
	codec => json
    }
}
	
filter {
	  grok {
   		 match => { "\A%{TIMESTAMP_ISO8601:time}\s*%{DATA:app}\s\[%{WORD:TraceId}?\,%{WORD:SpanId}?,%{WORD:ParentSpanId},%{WORD:spanExport}?\]\s*\[(?<threadname>[^\]]+)\]\s*%{LOGLEVEL:loglevel}\s*%{JAVACLASS:class}\s%{GREEDYDATA:message}\s*" }
	}

}
output {
    stdout{ codec => rubydebug } # 输出到控制台
      
    if "_grokparsefailure" in [tags] {
        # write events that didn't match to a file
        file { "path" => "/tmp/grok_failures.txt" }
    } else {
        elasticsearch { # 输出到 Elasticsearch
            action => "index"
            hosts  => ["es001:9200"]
            index  => "logstash-%{server_name}-%{+yyyy.MM.dd}"
            document_type => "%{server_name}"
            # user => "elastic" # 如果选择开启xpack security需要输入帐号密码
            # password => "changeme"
        }
      }
}   
